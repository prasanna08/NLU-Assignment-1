{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with Negative Sampling Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map each word to a unique Id and calculate word frequency, unigram distribution, subsampling frequency and negative sampling frequency of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import reuters\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import os\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus_words, ignore_threshold=5):\n",
    "    # Consider only alphabetic words.\n",
    "    words = [wrd.lower() for wrd in corpus_words if wrd.isalpha()]\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Ignore words which occur less than threshold times in all documents.\n",
    "    # Replace them by a special UNK word.\n",
    "    if ignore_threshold > 0:\n",
    "        unk_word = 'UNK'\n",
    "        unk_cnt = 0\n",
    "        unique_words = list(word_freq.keys())\n",
    "        for wrd in unique_words:\n",
    "            if word_freq[wrd] < ignore_threshold:\n",
    "                unk_cnt += word_freq[wrd]\n",
    "                del word_freq[wrd]\n",
    "    \n",
    "    # Create a Word to ID map.\n",
    "    total_words = sum(cnt for wrd, cnt in word_freq.items())\n",
    "    increment = 1 if ignore_threshold > 0 else 0\n",
    "    word_to_id = {wrd: (i+increment) for i, wrd in enumerate(word_freq)}\n",
    "    if ignore_threshold > 0:\n",
    "        word_to_id[unk_word] = 0\n",
    "    \n",
    "    # Generate Pn and Ps probabilities.\n",
    "    total_pn = sum(np.power(float(word_freq[wrd])/total_words, 0.75) for wrd in word_freq)\n",
    "    word_metadata = {\n",
    "        word_to_id[wrd]: {\n",
    "            'freq': word_freq[wrd],\n",
    "            'Uw': float(word_freq[wrd])/total_words,\n",
    "            'Ps': 1 - np.sqrt(1e-5 / (float(word_freq[wrd])/total_words)),\n",
    "            'Pn': np.power(float(word_freq[wrd])/total_words, 0.75) / total_pn,\n",
    "        } for wrd in word_freq\n",
    "    }\n",
    "    \n",
    "    if ignore_threshold > 0:\n",
    "        word_metadata[0] = {\n",
    "            'freq': unk_cnt, 'Uw': 0, 'Ps': 1 - np.sqrt(1e-5 / unk_cnt) , 'Pn': 0\n",
    "        }\n",
    "    \n",
    "    # For numerical stability, due to insufficient precision in float.\n",
    "    cdf = 0.0\n",
    "    for wrd in word_metadata:\n",
    "        cdf += word_metadata[wrd]['Pn']\n",
    "        word_metadata[wrd]['cdf'] = cdf\n",
    "    return word_to_id, word_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Batch Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Generator generates a minibatch to train. Output is (x, y) where x is input word and first word of y is target word whereas rest of the words in y are negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, corpus, batch_size, split_ratio=0.8, window_size=3, loaded_data=None):\n",
    "        self.corpus = corpus\n",
    "        self.ids = corpus.fileids()\n",
    "        self.batch_size = batch_size\n",
    "        self.corpus_words = self.corpus.words()\n",
    "        if loaded_data is None:\n",
    "            self.word_to_id, self.word_metadata = preprocess_corpus(self.corpus_words)\n",
    "        else:\n",
    "            self.word_to_id = loaded_data['word_to_id']\n",
    "            self.word_metadata = {int(k): v for k, v in loaded_data['word_metadata'].items()}\n",
    "        self.n_words = len(self.word_to_id)\n",
    "        self.split_ratio = split_ratio\n",
    "        self.window_size = window_size\n",
    "        self.n_steps = 0\n",
    "        self.splitted = False\n",
    "        self._cursor = self.window_size + 1\n",
    "        self.word_ids = [self.word_to_id[wrd] for wrd in self.corpus_words if wrd in self.word_to_id]\n",
    "        del self.corpus_words\n",
    "    \n",
    "    def set_batch_size(self, new_size):\n",
    "        self.batch_size = new_size\n",
    "    \n",
    "    def split(self):\n",
    "        self.shuffle_data()\n",
    "        self.splitted = True\n",
    "        instances = len(self.ids)\n",
    "        i = int(self.split_ratio*instances)\n",
    "        self.train_ids = self.ids[:i]\n",
    "        self.valid_ids = self.ids[i:]\n",
    "        self.train_word_ids = [self.word_to_id[wrd] for wrd in self.corpus.words(self.train_ids) if wrd in self.word_to_id]\n",
    "        self.n_train_words = len(self.train_word_ids)\n",
    "        self.validation_word_ids = [self.word_to_id[wrd] for wrd in self.corpus.words(self.train_ids) if wrd in self.word_to_id]\n",
    "        self.n_validation_words = len(self.validation_word_ids)\n",
    "    \n",
    "    def get_validation_data(self):\n",
    "        if self.splitted:\n",
    "            return self.valid_data, self.valid_labels\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._cursor = self.window_size + 1\n",
    "        return self\n",
    "    \n",
    "    def get_negative_probs(self):\n",
    "        return [self.word_metadata[wrd]['Pn'] for wrd in sorted(self.word_metadata.keys())]\n",
    "    \n",
    "    def _increment_cursor(self, max_len):\n",
    "        self._cursor = (self._cursor + 1) % (max_len)\n",
    "        if self._cursor == 0:\n",
    "            self._cursor = self.window_size + 1\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_metadata(cls, fname, corpus, batch_size, split_ratio=0.8, window_size=3):\n",
    "        f = open(fname, 'r')\n",
    "        data = json.loads(f.read())\n",
    "        f.close()\n",
    "        return cls(corpus, batch_size, split_ratio=split_ratio, window_size=window_size, loaded_data=data)\n",
    "        \n",
    "    def store_metadata(self, fname):\n",
    "        data = {\n",
    "            'word_to_id': self.word_to_id,\n",
    "            'word_metadata': self.word_metadata\n",
    "        }\n",
    "        f = open(fname, 'w')\n",
    "        f.write(json.dumps(data))\n",
    "        f.close()\n",
    "\n",
    "    def __next__(self):\n",
    "        word_ids = self.word_ids if not self.splitted else self.train_word_ids\n",
    "        batch_input = []\n",
    "        batch_output = []\n",
    "        Pds = np.random.uniform(size=(self.batch_size))\n",
    "        sample_cnt = 0\n",
    "        while len(batch_input) < self.batch_size:\n",
    "            wrd = word_ids[self._cursor]\n",
    "            \n",
    "            if Pds[sample_cnt] > self.word_metadata[wrd]['Ps']:\n",
    "                batch_input.append(wrd)\n",
    "                targets = word_ids[self._cursor - self.window_size: self._cursor]\n",
    "                epoch = self._increment_cursor(len(word_ids))\n",
    "                targets += word_ids[self._cursor: self._cursor + self.window_size]\n",
    "                batch_output.append(targets)\n",
    "            \n",
    "            sample_cnt += 1\n",
    "            if sample_cnt == self.batch_size:\n",
    "                Pds = np.random.uniform(size=(self.batch_size))\n",
    "                sample_cnt = 0\n",
    "        \n",
    "        return np.array(batch_input).reshape(-1, 1).astype(np.int32), np.array(batch_output), epoch\n",
    "    \n",
    "    def next(self):\n",
    "        return self.__next__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairBatchGenerator(BatchGenerator):\n",
    "    def __init__(self, corpus, batch_size, split_ratio=0.8, window_size=3, loaded_data=None):\n",
    "        super(PairBatchGenerator, self).__init__(corpus, batch_size, split_ratio=split_ratio, window_size=window_size, loaded_data=loaded_data)\n",
    "        self._generate_pairs()\n",
    "        self.shuffle()\n",
    "    \n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.pairs)\n",
    "        \n",
    "    def _generate_pairs(self):\n",
    "        word_pairs = defaultdict(set)\n",
    "        for idx, wrd in enumerate(self.word_ids):\n",
    "            p = np.random.uniform()\n",
    "            if p > self.word_metadata[wrd]['Ps']:\n",
    "                lidx = max(0, idx - self.window_size)\n",
    "                ridx = min(len(self.word_ids), idx + 1 + self.window_size)\n",
    "                word_pairs[wrd].update(self.word_ids[lidx:ridx])\n",
    "        \n",
    "        pairs = []\n",
    "        for wrd in word_pairs:\n",
    "            for owrd in word_pairs[wrd]:\n",
    "                pairs.append((wrd, owrd))\n",
    "        \n",
    "        self.pairs = pairs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._cursor = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._cursor + self.batch_size >= len(self.pairs):\n",
    "            rem = (self._cursor + self.batch_size) - len(self.pairs)\n",
    "            batch = np.array(self.pairs[self._cursor:] + self.pairs[:rem])\n",
    "            self._cursor = rem\n",
    "            return batch[:, 0].reshape(-1, 1), batch[:, 1].reshape(-1, 1), True\n",
    "        _pcursor = self._cursor\n",
    "        batch = np.array(self.pairs[self._cursor: self._cursor + self.batch_size])\n",
    "        self._cursor = (self._cursor + self.batch_size) % len(self.pairs)\n",
    "        return batch[:, 0].reshape(-1, 1), batch[:, 1].reshape(-1, 1), True if _pcursor >= self._cursor else False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = PairBatchGenerator(reuters, 256)\n",
    "#sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "stime = time.time()\n",
    "for (x, y, p) in bg:\n",
    "    if p:\n",
    "        etime = time.time()\n",
    "        print(x.shape, y.shape, p, etime-stime)\n",
    "        stime = etime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The skip-gram architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram architecture to train a word2vec model. Define two embedding matrices, one for input word (x) and other for target words in (y). Define negative sampling loss and optimize network over negative sampling loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(bg.word_to_id)\n",
    "embedding_dim = 128\n",
    "batch_size = bg.batch_size\n",
    "n_neg_samples = 8\n",
    "window_size = bg.window_size\n",
    "\n",
    "sampler = tf.distributions.Categorical(probs=bg.get_negative_probs())\n",
    "with tf.name_scope('Word2Vec-skipgram'):\n",
    "    with tf.name_scope('Inputs'):\n",
    "        inputs = tf.placeholder(dtype=tf.int32, shape=(None, 1), name='input-word')\n",
    "        targets = tf.placeholder(dtype=tf.int32, shape=(None, 2*window_size), name='output-words')\n",
    "        neg_samples = sampler.sample((bg.batch_size, n_neg_samples))\n",
    "    with tf.name_scope('Embeddings'):\n",
    "        U = tf.Variable(tf.truncated_normal((n_words, embedding_dim)), name='U')\n",
    "        V = tf.Variable(tf.truncated_normal((n_words, embedding_dim)), name='V')\n",
    "        B = tf.Variable(tf.constant(0., shape=(n_words,)), name='B')\n",
    "        hist_u = tf.summary.histogram('U', U)\n",
    "        hist_v = tf.summary.histogram('V', V)\n",
    "        hist_b = tf.summary.histogram('B', B)\n",
    "    with tf.name_scope('Score'):\n",
    "        E1 = tf.reshape(tf.nn.embedding_lookup(U, inputs), (-1, 1, embedding_dim))\n",
    "        E2 = tf.transpose((tf.nn.embedding_lookup(V, targets)), perm=[0, 2, 1])\n",
    "        E3 = tf.transpose(tf.negative(tf.nn.embedding_lookup(V, neg_samples)), perm=[0, 2, 1])\n",
    "        B2 = tf.expand_dims(tf.nn.embedding_lookup(B, targets), 1)\n",
    "        B3 = tf.expand_dims(tf.nn.embedding_lookup(B, neg_samples), 1)\n",
    "    with tf.name_scope('Loss'):\n",
    "        pos_loss = tf.reduce_sum(tf.log(tf.nn.sigmoid(tf.matmul(E1, E2) + B2)), axis=2)\n",
    "        neg_loss = tf.reduce_sum(tf.log(tf.nn.sigmoid(tf.matmul(E1, E3) + B3)), axis=2)\n",
    "        loss = tf.negative(tf.add(tf.reduce_mean(pos_loss, axis=0), tf.reduce_mean(neg_loss, axis=0)))\n",
    "        loss_summary = tf.summary.scalar('loss', loss[0])\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        global_step = tf.Variable(0, name='global_step')\n",
    "        opt = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)\n",
    "        merged_op = tf.summary.merge([loss_summary, hist_u, hist_v, hist_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A slightly different architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(bg.word_to_id)\n",
    "embedding_dim = 256\n",
    "batch_size = bg.batch_size\n",
    "n_neg_samples = 8\n",
    "window_size = bg.window_size\n",
    "\n",
    "with tf.name_scope('Word2Vec-skipgram'):\n",
    "    with tf.name_scope('Inputs'):\n",
    "        inputs = tf.placeholder(dtype=tf.int64, shape=(None, 1), name='input-word')\n",
    "        targets = tf.placeholder(dtype=tf.int64, shape=(None, 1), name='output-words')\n",
    "        neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
    "            true_classes=targets, num_true=1, num_sampled=n_neg_samples, unique=True,\n",
    "            range_max=n_words, unigrams=bg.get_negative_probs(), distortion=0.75)\n",
    "    with tf.name_scope('Embeddings'):\n",
    "        U = tf.Variable(tf.truncated_normal((n_words, embedding_dim)), name='U')\n",
    "        V = tf.Variable(tf.truncated_normal((n_words, embedding_dim)), name='V')\n",
    "    with tf.name_scope('Score'):\n",
    "        E1 = tf.reshape(tf.nn.embedding_lookup(U, inputs), (-1, embedding_dim))\n",
    "        E2 = tf.reshape(tf.nn.embedding_lookup(V, targets), (-1, embedding_dim))\n",
    "        E3 = tf.negative(tf.nn.embedding_lookup(V, neg_samples))\n",
    "    with tf.name_scope('Loss'):\n",
    "        pos_loss = tf.reduce_sum(tf.multiply(E1, E2), axis=1)\n",
    "        neg_loss = tf.reduce_sum(tf.nn.sigmoid(tf.matmul(E1, tf.transpose(E3))), axis=1)\n",
    "        loss = tf.negative(tf.add(tf.reduce_mean(pos_loss, axis=0), tf.reduce_mean(neg_loss, axis=0)))\n",
    "        loss_summary = tf.summary.scalar('loss', loss)\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        global_step = tf.Variable(0, name='global_step')\n",
    "        opt = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)\n",
    "        merged_op = tf.summary.merge([loss_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: -4.112\n",
      "Epoch %d 0\n",
      "Epoch %d 1\n",
      "Epoch %d 2\n",
      "Epoch %d 3\n",
      "Epoch %d 4\n",
      "Epoch %d 5\n"
     ]
    }
   ],
   "source": [
    "# Writer has to be initialzed before all variables are initialized.\n",
    "writer = tf.summary.FileWriter('train/NCE-%d-%d-%d' % (embedding_dim, batch_size, n_neg_samples), sess.graph)\n",
    "summary_steps = 20\n",
    "checkpoint_steps = 1\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "checkpoint_path = './checkpoint/NCE-%d-%d-%d/' % (embedding_dim, batch_size, n_neg_samples)\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "max_epoch = 6\n",
    "epoch = 0\n",
    "step = 0\n",
    "#try:\n",
    "for (x, y, p) in bg:\n",
    "    _, step_loss = sess.run([opt, loss], feed_dict={inputs:x, targets:y})\n",
    "    if p:\n",
    "        print(\"Epoch %d\" % epoch)\n",
    "        epoch += 1\n",
    "    if step % summary_steps == 0:\n",
    "        merged = sess.run([merged_op], feed_dict={inputs:x, targets:y})[0]\n",
    "        print(\"Step: %d, Loss: %.3f\" % (step, step_loss))\n",
    "        writer.add_summary(merged, step)\n",
    "    step += 1\n",
    "    if epoch % checkpoint_steps == 0:\n",
    "        saver.save(sess, '%s/word2vec-%d.ckpt' % (checkpoint_path, epoch))\n",
    "    if epoch >= max_epoch:\n",
    "        saver.save(sess, '%s/word2vec-%d.ckpt' % (checkpoint_path, epoch))\n",
    "        break\n",
    "#except Exception:\n",
    "#    saver.save(sess, '%s/word2vec-%d.ckpt' % (checkpoint_path, epoch))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x, y, _) = bg.__next__()\n",
    "#np.squeeze(x.reshape(1, -1)).reshape(-1, 1)\n",
    "#x.dtype, x.shape\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimLex - 999 Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/Trial 4/NCE-300-256-10/word2vec-20.ckpt\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "sess = tf.Session(graph=g)\n",
    "\n",
    "#best_model = './checkpoint/Trial 3/NCE-300-256-10/word2vec-20.ckpt'\n",
    "saver = tf.train.import_meta_graph('./checkpoint/Trial 4/NCE-300-256-10/word2vec-10.ckpt.meta', graph=g)\n",
    "\n",
    "# Restore checkpoint\n",
    "saver.restore(sess, './checkpoint/Trial 4/NCE-300-256-10/word2vec-20.ckpt')\n",
    "\n",
    "# Get embeddings matrix.\n",
    "graph = sess.graph\n",
    "U = graph.get_tensor_by_name('Word2Vec-skipgram/Embeddings/U:0')\n",
    "V = graph.get_tensor_by_name('Word2Vec-skipgram/Embeddings/V:0')\n",
    "\n",
    "Wu = np.array(sess.run([U])[0])\n",
    "Wv = np.array(sess.run([V])[0])\n",
    "Wu = Wu / np.linalg.norm(Wu, axis=1).reshape(-1, 1)\n",
    "Wv = Wv / np.linalg.norm(Wv, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10428, 300)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SimLex - 999 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./SimLex-999/SimLex-999.txt', 'r')\n",
    "pairs = [[line.split('\\t')[0], line.split('\\t')[1], float(line.split('\\t')[3])] for line in f.readlines()[1:]]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = BatchGenerator.load_from_metadata('bg-data-5.json', reuters, batch_size=128)\n",
    "\n",
    "get_idx = lambda x: bg.word_to_id[x] if x in bg.word_to_id else 0\n",
    "pair_ids = []\n",
    "for p in pairs:\n",
    "    if p[0] in bg.word_to_id and p[1] in bg.word_to_id:\n",
    "        pair_ids.append([get_idx(p[0]), get_idx(p[1]), p[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_scores = []\n",
    "for p in pair_ids:\n",
    "    word_to_vec_scores.append([np.dot(Wu[p[0]], Wu[p[1]]), p[2]])\n",
    "\n",
    "word_to_vec_scores = np.array(word_to_vec_scores).astype(np.float32)\n",
    "x = word_to_vec_scores[:,0]\n",
    "y = word_to_vec_scores[:,1]\n",
    "\n",
    "l = spearmanr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15695845562342065\n"
     ]
    }
   ],
   "source": [
    "print(l.correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model accuracy on word analogy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = dict(zip(bg.word_to_id.values(), bg.word_to_id.keys()))\n",
    "def top_k_closest_words(wrd, k=8):\n",
    "    idx = bg.word_to_id[wrd]\n",
    "    emb = Wu[idx]\n",
    "    idcs = np.argsort(np.abs(np.dot(emb, Wu.T)))[-k:]\n",
    "    return [id_to_word[i] for i in idcs[::-1]]\n",
    "\n",
    "def word_analogy(wrd1, wrd2, wrd3, k=1):\n",
    "    idx1 = bg.word_to_id[wrd1]\n",
    "    idx2 = bg.word_to_id[wrd2]\n",
    "    idx3 = bg.word_to_id[wrd3]\n",
    "    emb = Wu[idx1] - Wu[idx2] + Wu[idx3]\n",
    "    idcs = np.argsort(np.abs(np.dot(emb, Wu.T)))[-k:]\n",
    "    return [id_to_word[i] for i in idcs[::-1]]\n",
    "\n",
    "def sim(wrd1, wrd2):\n",
    "    idx1 = bg.word_to_id[wrd1]\n",
    "    idx2 = bg.word_to_id[wrd2]\n",
    "    return np.dot(Wu[idx1], Wu[idx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('questions-words.txt', 'r')\n",
    "data = [line.split() for line in f.readlines() if not line.startswith(':')]\n",
    "syntactic = data[:8869]\n",
    "semantic = data[8869:]\n",
    "\n",
    "def get_accuracy(data, bg):\n",
    "    correct = 0\n",
    "    cnt = 0\n",
    "    for d in data:\n",
    "        if all([d[0].lower() in bg.word_to_id, d[1].lower() in bg.word_to_id, d[2].lower() in bg.word_to_id, d[3].lower() in bg.word_to_id]):\n",
    "            cnt += 1\n",
    "            if d[3].lower() in word_analogy(d[0].lower(), d[1].lower(), d[2].lower()):\n",
    "                correct +=1\n",
    "    return correct, cnt\n",
    "\n",
    "\n",
    "get_accuracy(data, bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative results for word analogy taks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20896176, 0.14953859)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_analogy('male', 'female', '')\n",
    "# Biases transport (cheap vs costly), partnership (male vs female), income (male vs female), professor (banking), reporters (male vs female), nations (developed vs developing)\n",
    "# Professional (male vs female), engineering (man vs female), producer (male vs female), rich (male vs female), loving (male vs female)\n",
    "# General male vs female is quite interesting.\n",
    "#top_k_closest_words('male')\n",
    "sim('bank', 'money'), sim('bank', 'welfare')\n",
    "#word_analogy('sports', 'team', 'army')\n",
    "# Quantitative word analogy\n",
    "# Man, Engineering, Female, Secretariat\n",
    "# Male, Loving, Female, Dramatic\n",
    "# Fianance, Bank, Court, Smuggling\n",
    "# Banker, Bank, Professor, Science\n",
    "# Barclays (men), income (tax), engineering (appliances, equipment), court (lawsuite, judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Graphs for various values of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General function for plotting graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def parse_file(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        data = [float(line.split()[-1]) for line in f.readlines()]\n",
    "    return data\n",
    "\n",
    "def plot(fnames, name, mode=None):\n",
    "    data = {l: parse_file(fname) for l, fname in fnames.items()}\n",
    "    if mode == 'batch size':\n",
    "        data = {d: data[d][::(2-i) * 2 if (2-i) * 2 > 0 else 1] for i, d in enumerate(data)}\n",
    "    for d in data:\n",
    "        plt.plot(range(len(data[d])), data[d], label=d)\n",
    "    plt.legend()\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(name)\n",
    "    plt.savefig('%s-plot.png' % (name))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'Batch Size': {\n",
    "        'batch size 128': './Reports/Batch Size/Word2Vec-batch-128.txt',\n",
    "        'batch size 256': './Reports/Batch Size/Word2Vec-batch-256.txt',\n",
    "        'batch size 512': './Reports/Batch Size/Word2Vec-batch-512.txt'\n",
    "    },\n",
    "    'Skip Window': {\n",
    "        'windows size 3': './Reports/Skip window/Word2Vec-skip-window-3.txt',\n",
    "        'windows size 6': './Reports/Skip window/Word2Vec-skip-window-6.txt',\n",
    "        'windows size 10': './Reports/Skip window/Word2Vec-skip-window-10.txt',\n",
    "        \n",
    "    },\n",
    "    'Negative Samples': {\n",
    "        'neg samples 8': './Reports/Negative Samples/Word2Vec_neg_samples-8.txt',\n",
    "        'neg samples 12': './Reports/Negative Samples/Word2Vec_neg_samples-12.txt',\n",
    "        'neg samples 20': './Reports/Negative Samples/Word2Vec_neg_samples-20.txt',\n",
    "        \n",
    "    },\n",
    "    'Embedding Dims': {\n",
    "        'Embedding Dim 128': './Reports/Embedding dims/Word2Vec-embedding-dim-128.txt',\n",
    "        'Embedding Dim 256': './Reports/Embedding dims/Word2Vec-embedding-dim-256.txt',\n",
    "        'Embedding Dim 300': './Reports/Embedding dims/Word2Vec-embedding-dim-300.txt',\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(files['Batch Size'], 'Batch Size', mode='batch size')\n",
    "plot(files['Skip Window'], 'Skip window')\n",
    "plot(files['Negative Samples'], 'Negative Samples')\n",
    "plot(files['Embedding Dims'], 'Embedding Dims')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
